{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVI-DET.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WALAEVKi25vu"
      },
      "source": [
        "# **COVI-DET**\n",
        "\n",
        "The following notebook implements a Deep Neural Network model, whose backbone is a Dense-Net based architecture known as CheXNet. CheXNet is a model trained on Pneumonia X-Rays, which gives better performance than radiologists. We apply transfer learning on this model to the COVID-19 dataset to detect COVID-19 from X-Ray Images. We also apply RISE (Randomized Input Sampling for Explanation of Black-box Models) to generate Saliency maps for model interpretability. \n",
        "\n",
        "## Dataset Sources\n",
        "\n",
        "\n",
        "\n",
        "*   COVID-19 Chest X-Ray Dataset : https://github.com/ieee8023/covid-chestxray-dataset\n",
        "*   Pneumonia Chest X-Ray Dataset : https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n",
        "*   Pre-trained weights for CheXNet : https://github.com/arnoweng/CheXNet\n",
        "\n",
        "\n",
        "\n",
        "We also use the Pneumonia Chest X-Ray dataset, because the class frequencies for COVID-19 dataset has fewer images for Pneumonia and Normal X-Rays. We combine them to form our dataset, which will be further split into training, validation and test sets.\n",
        "\n",
        "## References\n",
        "\n",
        "\n",
        "\n",
        "*   https://arxiv.org/abs/2004.12823\n",
        "*   https://arxiv.org/abs/2004.09803\n",
        "*   https://github.com/arnoweng/CheXNet\n",
        "*   https://stanfordmlgroup.github.io/projects/chexnet/\n",
        "*   https://github.com/eclique/RISE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B6dpqNcojZB"
      },
      "source": [
        "# Imports and Data Downloading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u1jbAhLZcpx"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\n",
        "import re\n",
        "from shutil import copyfile\n",
        "import glob\n",
        "import warnings\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc, f1_score\n",
        "from skimage.transform import resize\n",
        "\n",
        "from PIL import Image\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('max_colwidth', 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmou2G2M5sqD"
      },
      "source": [
        "## Data Downloading\n",
        "\n",
        "The following cell clones my repo, which contains the dataset for this task. The data present in the repo is as it is downloaded from the above mentioned Data Sources and stored at one place (cannot download from Kaggle without a private API token). All the pre-processing will be done in this notebook only.\n",
        "\n",
        "The repo also contains the pretrained weights for CheXNet Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suLhL0_voZ4e"
      },
      "source": [
        "!git clone https://github.com/dragonsan17/covid_detection_from_xray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpC7t_0EcOVV"
      },
      "source": [
        "!mkdir final_data\n",
        "!mkdir final_data/train\n",
        "!mkdir final_data/validation\n",
        "!mkdir final_data/test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3NTWocPouzF"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB0sk1QZMZos"
      },
      "source": [
        "CKPT_PATH = '/content/covid_detection_from_xray/data/chexnet_pretrained'\n",
        "METADATA_PATH = '/content/covid_detection_from_xray/data/covid-chestxray-dataset/metadata.csv'\n",
        "\n",
        "TRAIN_DATA_PATH = '/content/final_data/train'\n",
        "VAL_DATA_PATH = '/content/final_data/validation'\n",
        "TEST_DATA_PATH = '/content/final_data/test'\n",
        "SAVE_PATH = '/content/final_data'\n",
        "\n",
        "NORMAL_DATA_PATH = \"/content/covid_detection_from_xray/data/NORMAL\"\n",
        "PNEUMONIDA_DATA_PATH = \"/content/covid_detection_from_xray/data/PNEUMONIA\"\n",
        "COVID_DATA_PATH = '/content/covid_detection_from_xray/data/covid-chestxray-dataset'\n",
        "\n",
        "BEST_PATH = CKPT_PATH\n",
        "BEST_VAL = 100000\n",
        "\n",
        "NUM_EPOCHS_FIRST_RUN = 30 #1 Replace by 1 to see the functioning faster\n",
        "NUM_EPOCHS_SECOND_RUN = 10 #1 Replace by 1 to see the functioning faster\n",
        "BATCH_SIZE_FIRST_RUN = 16\n",
        "BATCH_SIZE_SECOND_RUN = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAz46xbKBzg9"
      },
      "source": [
        "# Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pu8_462eKEr"
      },
      "source": [
        "img_paths = []\n",
        "classes = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xBmQb-mjcSG"
      },
      "source": [
        "## Data Reading for Normal and Pneumonia X-Rays\n",
        "\n",
        "Reads the images from their respective folders and splits into train-val-test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtDSA1yNSXGI"
      },
      "source": [
        "for f in glob.glob(os.path.join(NORMAL_DATA_PATH, '*')):\n",
        "  img_paths.append(f)\n",
        "  classes.append(0)\n",
        "\n",
        "for f in glob.glob(os.path.join(PNEUMONIDA_DATA_PATH, '*')):\n",
        "  if 'bacteria' in f:\n",
        "    img_paths.append(f)\n",
        "    classes.append(1)\n",
        "  else:\n",
        "    img_paths.append(f)\n",
        "    classes.append(2)\n",
        "\n",
        "chest_xray_data = pd.DataFrame({'img_paths' : img_paths, 'classes' : classes})\n",
        "chest_xray_train, chest_xray_test, _, _ = train_test_split(chest_xray_data, chest_xray_data.classes, test_size=0.3, random_state=42, stratify=chest_xray_data.classes)\n",
        "chest_xray_valid, chest_xray_test, _, _ = train_test_split(chest_xray_test, chest_xray_test.classes, test_size=0.33, random_state=42, stratify=chest_xray_test.classes)\n",
        "\n",
        "train_img_paths = list(chest_xray_train.img_paths)\n",
        "train_classes = list(chest_xray_train.classes)\n",
        "\n",
        "val_img_paths = list(chest_xray_valid.img_paths)\n",
        "val_classes = list(chest_xray_valid.classes)\n",
        "\n",
        "test_img_paths = list(chest_xray_test.img_paths)\n",
        "test_classes = list(chest_xray_test.classes)\n",
        "\n",
        "for index, row in chest_xray_train.iterrows():\n",
        "  src = row.img_paths\n",
        "  img_name = src.split('/')[-1]\n",
        "  dst = os.path.join(TRAIN_DATA_PATH, img_name)\n",
        "  copyfile(src,dst)\n",
        "\n",
        "for index, row in chest_xray_valid.iterrows():\n",
        "  src = row.img_paths\n",
        "  img_name = src.split('/')[-1]\n",
        "  dst = os.path.join(VAL_DATA_PATH, img_name)\n",
        "  copyfile(src,dst)\n",
        "\n",
        "for index, row in chest_xray_test.iterrows():\n",
        "  src = row.img_paths\n",
        "  img_name = src.split('/')[-1]\n",
        "  dst = os.path.join(TEST_DATA_PATH, img_name)\n",
        "  copyfile(src,dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU9ylC4ejic3"
      },
      "source": [
        "## Data Reading for COVID-19 X-Rays\n",
        "\n",
        "\n",
        "*   Reads the Metadata and only chooses COVID-19 images\n",
        "*   Due to the presence of multiple images of same patient-id, the train-val-test split is made in such a way that same patient-id's image does not fall into train and the others, thus preventing information leakage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGwQNrjeB2XI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "64687fb7-3ed6-4c45-e9eb-1ad426d6962f"
      },
      "source": [
        "covid_data = pd.read_csv(METADATA_PATH).fillna('')\n",
        "covid_data = covid_data[((covid_data.view == 'PA') | (covid_data.view == 'AP') | (covid_data.view == 'AP Supine')) & ((covid_data.finding == 'Pneumonia/Viral/COVID-19'))]\n",
        "covid_data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patientid</th>\n",
              "      <th>offset</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>finding</th>\n",
              "      <th>RT_PCR_positive</th>\n",
              "      <th>survival</th>\n",
              "      <th>intubated</th>\n",
              "      <th>intubation_present</th>\n",
              "      <th>went_icu</th>\n",
              "      <th>in_icu</th>\n",
              "      <th>needed_supplemental_O2</th>\n",
              "      <th>extubated</th>\n",
              "      <th>temperature</th>\n",
              "      <th>pO2_saturation</th>\n",
              "      <th>leukocyte_count</th>\n",
              "      <th>neutrophil_count</th>\n",
              "      <th>lymphocyte_count</th>\n",
              "      <th>view</th>\n",
              "      <th>modality</th>\n",
              "      <th>date</th>\n",
              "      <th>location</th>\n",
              "      <th>folder</th>\n",
              "      <th>filename</th>\n",
              "      <th>doi</th>\n",
              "      <th>url</th>\n",
              "      <th>license</th>\n",
              "      <th>clinical_notes</th>\n",
              "      <th>other_notes</th>\n",
              "      <th>Unnamed: 29</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "      <td>478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>295</td>\n",
              "      <td>40</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>29</td>\n",
              "      <td>34</td>\n",
              "      <td>15</td>\n",
              "      <td>23</td>\n",
              "      <td>22</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>55</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>478</td>\n",
              "      <td>77</td>\n",
              "      <td>215</td>\n",
              "      <td>9</td>\n",
              "      <td>342</td>\n",
              "      <td>104</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>250</td>\n",
              "      <td></td>\n",
              "      <td>M</td>\n",
              "      <td></td>\n",
              "      <td>Pneumonia/Viral/COVID-19</td>\n",
              "      <td>Y</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>PA</td>\n",
              "      <td>X-ray</td>\n",
              "      <td>2020</td>\n",
              "      <td>Hannover Medical School, Hannover, Germany</td>\n",
              "      <td>images</td>\n",
              "      <td>333932bd.jpg</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/ml-workgroup/covid-19-image-repository</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>7</td>\n",
              "      <td>92</td>\n",
              "      <td>287</td>\n",
              "      <td>125</td>\n",
              "      <td>478</td>\n",
              "      <td>284</td>\n",
              "      <td>309</td>\n",
              "      <td>318</td>\n",
              "      <td>318</td>\n",
              "      <td>237</td>\n",
              "      <td>285</td>\n",
              "      <td>416</td>\n",
              "      <td>447</td>\n",
              "      <td>418</td>\n",
              "      <td>385</td>\n",
              "      <td>464</td>\n",
              "      <td>454</td>\n",
              "      <td>442</td>\n",
              "      <td>196</td>\n",
              "      <td>478</td>\n",
              "      <td>320</td>\n",
              "      <td>79</td>\n",
              "      <td>478</td>\n",
              "      <td>1</td>\n",
              "      <td>231</td>\n",
              "      <td>79</td>\n",
              "      <td>205</td>\n",
              "      <td>97</td>\n",
              "      <td>272</td>\n",
              "      <td>478</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       patientid offset  sex  ... clinical_notes other_notes Unnamed: 29\n",
              "count        478    478  478  ...            478         478         478\n",
              "unique       295     40    3  ...            342         104           1\n",
              "top          250           M  ...                                       \n",
              "freq           7     92  287  ...             97         272         478\n",
              "\n",
              "[4 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqAcxEDjNVti",
        "outputId": "0eb17a59-e302-452d-cbb0-ac86dca3871a"
      },
      "source": [
        "unique_patient_ids = np.array(covid_data.patientid.unique())\n",
        "patient_id_counts = []\n",
        "total_samples = len(covid_data)\n",
        "for patient_id in unique_patient_ids:\n",
        "  count = len(covid_data[covid_data.patientid==patient_id])\n",
        "  patient_id_counts.append([count, patient_id])\n",
        "\n",
        "patient_id_counts.sort(reverse = True)\n",
        "\n",
        "train_patient_ids = []\n",
        "val_patient_ids = []\n",
        "test_patient_ids = []\n",
        "t_c, v_c, te_c = 0,0,0\n",
        "total_count = 0\n",
        "for count,id in patient_id_counts:\n",
        "  total_count += count\n",
        "  if total_count < 0.7*total_samples:\n",
        "    train_patient_ids.append(id)\n",
        "    t_c += count\n",
        "  elif total_count < 0.9*total_samples:\n",
        "    val_patient_ids.append(id)\n",
        "    v_c += count\n",
        "  else:\n",
        "    test_patient_ids.append(id)\n",
        "    te_c += count\n",
        "\n",
        "print(f'Total Samples : {total_samples}')\n",
        "print(f'Training Set contains {(t_c)} samples')\n",
        "print(f'Validation Set contains {v_c} samples')\n",
        "print(f'Test Set contains {te_c} samples')\n",
        "\n",
        "for patient_id in train_patient_ids:\n",
        "  details = covid_data[covid_data.patientid == patient_id]\n",
        "  filenames = details.filename\n",
        "\n",
        "  for filename in filenames:\n",
        "    src = os.path.join(COVID_DATA_PATH, filename)\n",
        "    dst = os.path.join(TRAIN_DATA_PATH, filename)\n",
        "    img_paths.append(filename)\n",
        "    classes.append(3)\n",
        "    train_img_paths.append(filename)\n",
        "    train_classes.append(3)\n",
        "    copyfile(src, dst)\n",
        "\n",
        "for patient_id in val_patient_ids:\n",
        "  details = covid_data[covid_data.patientid == patient_id]\n",
        "  filenames = details.filename\n",
        "\n",
        "  for filename in filenames:\n",
        "    src = os.path.join(COVID_DATA_PATH, filename)\n",
        "    dst = os.path.join(VAL_DATA_PATH, filename)\n",
        "    img_paths.append(filename)\n",
        "    classes.append(3)\n",
        "    \n",
        "    val_img_paths.append(filename)\n",
        "    val_classes.append(3)\n",
        "    copyfile(src, dst)\n",
        "\n",
        "for patient_id in test_patient_ids:\n",
        "  details = covid_data[covid_data.patientid == patient_id]\n",
        "  filenames = details.filename\n",
        "\n",
        "  for filename in filenames:\n",
        "    src = os.path.join(COVID_DATA_PATH, filename)\n",
        "    dst = os.path.join(TEST_DATA_PATH, filename)\n",
        "    img_paths.append(filename)\n",
        "    classes.append(3)\n",
        "    test_img_paths.append(filename)\n",
        "    test_classes.append(3)\n",
        "    copyfile(src, dst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Samples : 478\n",
            "Training Set contains 334 samples\n",
            "Validation Set contains 96 samples\n",
            "Test Set contains 48 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-WQpvYEfOlD"
      },
      "source": [
        "## Data-Set Class\n",
        "\n",
        "This class will be responsible to supply data, and also has an loss function as a member, which implements binary-weighted crossentropy loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYu3QtlTXmtT"
      },
      "source": [
        "all_data = pd.DataFrame({'img_paths' : img_paths, 'classes' : classes})\n",
        "all_data.img_paths = all_data.img_paths.transform(lambda x : str(x).split('/')[-1])\n",
        "\n",
        "train_df = pd.DataFrame({'img_paths' : train_img_paths, 'classes' : train_classes})\n",
        "train_df.img_paths = train_df.img_paths.transform(lambda x : str(x).split('/')[-1])\n",
        "\n",
        "val_df = pd.DataFrame({'img_paths' : val_img_paths, 'classes' : val_classes})\n",
        "val_df.img_paths = val_df.img_paths.transform(lambda x : str(x).split('/')[-1])\n",
        "\n",
        "test_df = pd.DataFrame({'img_paths' : test_img_paths, 'classes' : test_classes})\n",
        "test_df.img_paths = test_df.img_paths.transform(lambda x : str(x).split('/')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7ncf0IxtOWu"
      },
      "source": [
        "class Data_Set(Dataset):\n",
        "    def __init__(self, df, rand=False, transform=None):\n",
        "\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.rand = rand\n",
        "        self.transform = transform\n",
        "        self.num_normal = len(df[df.classes == 0])\n",
        "        self.num_bact = len(df[df.classes == 1])\n",
        "        self.num_viral = len(df[df.classes == 2])\n",
        "        self.num_covid = len(df[df.classes == 3])\n",
        "        self.total = len(df)\n",
        "        self.loss_weight_minus = torch.FloatTensor([self.num_normal, self.num_bact, self.num_viral, self.num_covid]).unsqueeze(0).cuda() / self.total\n",
        "        self.loss_weight_plus = 1.0 - self.loss_weight_minus\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        img_path = row.img_paths\n",
        "        path = ''\n",
        "        if os.path.exists(os.path.join(TRAIN_DATA_PATH, img_path)):\n",
        "          path = os.path.join(TRAIN_DATA_PATH, img_path)\n",
        "        elif os.path.exists(os.path.join(VAL_DATA_PATH, img_path)):\n",
        "          path = os.path.join(VAL_DATA_PATH, img_path)\n",
        "        else:\n",
        "          path = os.path.join(TEST_DATA_PATH, img_path)\n",
        "\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        label = np.zeros(4).astype(np.float32)\n",
        "        label[row.classes] = 1.\n",
        "        return torch.tensor(image).float(), torch.tensor(label)\n",
        "\n",
        "    def loss(self, output, target):\n",
        "        \n",
        "        weight_plus = torch.autograd.Variable(self.loss_weight_plus.repeat(1, target.size(0)).view(-1, self.loss_weight_plus.size(1)).cuda())\n",
        "        weight_neg = torch.autograd.Variable(self.loss_weight_minus.repeat(1, target.size(0)).view(-1, self.loss_weight_minus.size(1)).cuda())\n",
        "\n",
        "        loss = output\n",
        "        pmask = (target >= 0.5).data\n",
        "        nmask = (target < 0.5).data\n",
        "        \n",
        "        epsilon = 1e-15\n",
        "        loss[pmask] = (loss[pmask] + epsilon).log() * weight_plus[pmask]\n",
        "        loss[nmask] = (1-loss[nmask] + epsilon).log() * weight_plus[nmask]\n",
        "        loss = -loss.sum()\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}